# -*- coding: utf-8 -*-
"""GAN_Team4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ki24eHoTMRaWFrBgpA3Sh_fXc0XAcMo3
"""


import tensorflow as tf


import glob
import imageio
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
from tensorflow.keras import layers
import time

from IPython import display
from PIL import Image
import tensorflow as tf



#!pip install -q google-colab

#from google.colab import drive
#drive.mount('/content/gdrive')

folder_path = './project/360p'

import os

image_files = os.listdir(folder_path)

print(image_files)

"""from google.colab import files

# Use the files.upload() function to upload a file
uploaded = files.upload()
"""




# Check if the folder exists
if os.path.exists(folder_path):
    # List the files in the folder
    image_files = os.listdir(folder_path)
else:
    print(f"Folder '{folder_path}' does not exist.")

train_imgs_dir = folder_path
train_images = []

# List all files in the directory
image_files = [f for f in os.listdir(train_imgs_dir) if f.endswith('.jpg')]

for image_filename in image_files:
    image_path = os.path.join(train_imgs_dir, image_filename)

    # Open and append the image to the list
    image = Image.open(image_path)
    train_images.append(np.array(image))  # Convert to NumPy array

# Convert the list of arrays into a single NumPy array
train_images = np.array(train_images)

# Now, you can reshape and normalize the NumPy array
train_images = train_images.reshape(train_images.shape[0], 360, 360, 3).astype('float32')
train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]

BUFFER_SIZE=1200
BATCH_SIZE=32
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(5*5*256, use_bias=False, input_shape=(128,)))
    #model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((5, 5, 256)))
    assert model.output_shape == (None, 5, 5, 256)  # Note: None is the batch size


    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 5, 5, 128)
    model.add(layers.BatchNormalization())
   
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(3, 3), padding='same', use_bias=False))
    assert model.output_shape == (None, 15, 15, 64)
    model.add(layers.BatchNormalization())
    
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(3, 3), padding='same', use_bias=False))
    assert model.output_shape == (None, 45, 45, 32)
    model.add(layers.BatchNormalization())
    
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 90, 90, 16)
    model.add(layers.BatchNormalization())
        
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(8, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 180, 180, 8)
    #model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 360, 360, 3)


    # Initialize the weights of the generator using Xavier initialization
    for layer in model.layers:
        layer.kernel_initializer = tf.keras.initializers.GlorotUniform()


    return model

generator = make_generator_model()

noise = tf.random.normal([1, 128])
generated_image = generator(noise, training=False)

# Display the RGB image
plt.imshow(generated_image[0])

def make_discriminator_model():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                                     input_shape=[360, 360, 3]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.4))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.4))

    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.4))

    model.add(layers.Conv2D(512, (5, 5), strides=(3, 3), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.4))

    model.add(layers.Conv2D(1024, (5, 5), strides=(3, 3), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.4))

    model.add(layers.Flatten())
    model.add(layers.Dense(1))

    return model

discriminator = make_discriminator_model()
decision = discriminator(generated_image)
print (decision)

# This method returns a helper function to compute cross entropy loss
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00015, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

EPOCHS = 7000
noise_dim = 128
num_examples_to_generate = 16

# You will reuse this seed overtime (so it's easier)
# to visualize progress in the animated GIF)
seed = tf.random.normal([num_examples_to_generate, noise_dim])

# Notice the use of `tf.function`
# This annotation causes the function to be "compiled".
#Create empty lists to store the losses for the discriminator and generator during training:


@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = generator(noise, training=True)

      real_output = discriminator(images, training=True)
      fake_output = discriminator(generated_images, training=True)

      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,
                                                discriminator.trainable_variables))
                                                
  

import os

# Create a directory to save generated images
if not os.path.exists('generated_images'):
    os.mkdir('generated_images')

def generate_and_save_images(model, epoch, test_input, save_path):
    predictions = model(test_input, training=False)

    fig, axs = plt.subplots(4, 4, figsize=(4, 4))
    fig.suptitle('Generated Images')

    for i in range(predictions.shape[0]):
        row = i // 4
        col = i % 4
        display_image = tf.cast((predictions[i, :, :, :] * 127.5 + 127.5), tf.uint8)
        axs[row, col].imshow(display_image)
        axs[row, col].axis('off')

    # Save the generated image and display the plot
    plt.savefig(os.path.join(save_path, 'image_at_epoch_{:04d}.jpg'.format(epoch)))
    plt.show()


# In[15]:


import time

# Modify the training loop to save generated images and measure time
def train(dataset, epochs):
    gen_loss_list = []
    disc_loss_list = []

    for epoch in range(epochs):
        start = time.time()  # Record the start time for the current epoch

        for image_batch in dataset:
            train_step(image_batch)

        # Calculate losses for the epoch
        noise = tf.random.normal([BATCH_SIZE, noise_dim])
        generated_images = generator(noise, training=False)
        real_output = discriminator(image_batch, training=False)
        fake_output = discriminator(generated_images, training=False)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

        gen_loss_list.append(gen_loss)
        disc_loss_list.append(disc_loss)

        # Save the model every 15 epochs
        if (epoch + 1) % 15 == 0:
            checkpoint.save(file_prefix=checkpoint_prefix)

        elapsed_time = time.time() - start
        print('Time for epoch {} is {} sec'.format(epoch + 1, elapsed_time))

        # Generate and save images at the end of each epoch
        display.clear_output(wait=True)
        generate_and_save_images(generator, epoch, seed, 'generated_images')

    return gen_loss_list, disc_loss_list


# In[16]:


# Train the GAN
gen_loss_list, disc_loss_list = train(train_dataset, EPOCHS)


# In[17]:


# Create a plot of loss versus epochs
plt.figure()
plt.plot(gen_loss_list, label='Generator Loss')
plt.plot(disc_loss_list, label='Discriminator Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
plt.savefig('loss_vs_epochs.png', dpi=300)

